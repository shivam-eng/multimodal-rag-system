Multimodal Search & RAG System (Text â†” Image)
ğŸ“Œ Overview

This project demonstrates an end-to-end Multimodal Retrieval-Augmented Generation (RAG) system that enables semantic search across text and images and generates grounded answers using retrieved visual context.

The system uses contrastive multimodal embeddings (CLIP) to align text and images into a shared vector space, FAISS for fast similarity search, and an LLM-based (or local fallback) generation layer for response synthesis.

This project is intentionally scoped as a production-style prototype suitable for interviews, demos, and learning modern AI system design.

ğŸ§  Key Concepts Demonstrated

Multimodal embeddings using CLIP (text â†” image alignment)

Any-to-any semantic search (Text â†’ Image)

Vector similarity search with FAISS

Retrieval-Augmented Generation (RAG)

Graceful fallback when LLM APIs are unavailable

Clean, modular Python project structure

ğŸ—ï¸ Architecture
User Query
   â†“
Text Embedding (CLIP)
   â†“
FAISS Vector Search
   â†“
Top-K Relevant Images
   â†“
Context Augmentation
   â†“
LLM / Local Fallback
   â†“
Final Answer
ğŸ“‚ Project Structure
multimodal-rag/
â”‚
â”œâ”€â”€ app.py              # Application entry point
â”œâ”€â”€ embed.py            # Image embedding & FAISS index creation
â”œâ”€â”€ search.py           # Multimodal similarity search
â”œâ”€â”€ rag.py              # RAG generation logic (LLM + fallback)
â”œâ”€â”€ image_index.faiss   # FAISS vector index (generated)
â”œâ”€â”€ image_paths.npy     # Image path mapping (generated)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ images/         # Sample images (multiple categories)
â”‚
â”œâ”€â”€ .env.example        # Environment variable template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
ğŸ–¼ï¸ Dataset

10â€“20 heterogeneous images

Multiple categories (animals, objects, documents, charts)

No manual labels required (CLIP is zero-shot)

Example images:

cat.jpg

car.jpg

invoice.png

flowchart.png

âš™ï¸ Setup Instructions
1ï¸âƒ£ Create Virtual Environment
python -m venv .venv
.venv\Scripts\activate   # Windows
2ï¸âƒ£ Install Dependencies
pip install torch torchvision faiss-cpu pillow numpy python-dotenv
pip install git+https://github.com/openai/CLIP.git
ğŸ” Environment Variables

Create a .env file (optional):

OPENAI_API_KEY=your_api_key_here

âš ï¸ If no API key or quota is available, the system automatically switches to fallback mode.

ğŸš€ How to Run
Step 1: Generate Image Embeddings
python embed.py

This creates:

image_index.faiss

image_paths.npy

Step 2: Run the Application
python app.py

Example query:

cat sleeping on sofa
ğŸ”„ Fallback Mode (No API / No Quota)

If the LLM API is unavailable, the system:

Skips external generation

Returns a structured explanation based on retrieved context

This ensures the retrieval pipeline remains fully demonstrable.

ğŸ§ª Sample Output
Retrieved Images:
- data/images/cat.jpg
- data/images/sofa.jpg


Generated Answer:
The retrieved images show a cat resting comfortably on a sofa...
ğŸ” Limitations & Improvements

Small dataset may cause partial mismatches

Can be improved using caption-based hybrid search

Can be extended to audio/video modalities

ğŸš€ Future Enhancements

Image captioning + hybrid RAG

OCR for invoices and documents

Local LLM integration (Ollama)

Streamlit-based UI

Cloud deployment (GCP / AWS)
